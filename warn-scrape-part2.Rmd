---
title: "WARN Scrape"
author: "Helen Wieffering"
date: "3/30/2020"
output: html_document
---

# Intro

# I promise I will clean up this whole document soon.

# My data is scraped -- and therefore, current -- as of March 30, 2020

In brief: we want to scrape the WARN postings on the state website. This will tell us which companies (of 100+ employees) are planning layoffs in the economic downturn related to covid-19. Sarah gave me some code to get started on this, a lot of which I copied below. Her full instructions are in a file called `warn-scrape-part1.html`

```{r message=FALSE}

# load libraries
library(tidyverse)   #for everything! 
library(readr)       #for good reading of tsv
library(rvest)       #this is to read the html files.
library(stringr)     #this is to get rid of extra whitespace and work with strings
library(lubridate)   #this is to convert text to dates
library(ggplot2)     #for quick viz

```

Here's the link that houses the data. Note the different 'headers' in the URL. 
(to do - describe in more detail how we got to this link)
https://www.azjobconnection.gov/ada/mn_warn_dsp.cfm?securitysys=on&start_row=1&max_rows=25&orderby=employer&choice=1

EDIT: I should use this link instead, which sorts descending by date:
https://www.azjobconnection.gov/ada/mn_warn_dsp.cfm?securitysys=on&start_row=1&max_rows=25&orderby=noticeDateSort%20DESC&choice=1

If you page through the results, you see two things: a maximum of 25 rows can be displayed at a time. Typing over the 25 in the URL doesn't change that. In fact, it appears to be one giant table - with many rows. Page 2 shows rows 26-50, for example, and page 3 shows rows 51-75. There are a total of 379 rows over 16 pages as of March 30, 2020.

Our headers are set to the following:
```{r, eval=FALSE}

securitysys = on
start_row = 1
max_row = 25
orderby = employer # were I to do this again, I would sort by noticeDateSort%20DESC
choice = 1

```

I'll download the first page of results. THIS IS A TEST RUN. ALl my code chunks that hit the website will have eval set to false.

```{r, eval = FALSE}

download.file( "https://www.azjobconnection.gov/ada/mn_warn_dsp.cfm?securitysys=on&start_row=1&max_rows=25&orderby=employer&choice=1", "warn_listing/page001.html" )

```

TEST RUN WORKED. Ok, now to run a look to download every page of results.

Quickly, I want to better understand sprintf( ) works with these inputs. Ok, so the 03 must refer to the padding of the zero. The 'd' means a digit, which is where my p value gets dropped. And the % just means the start of an expressions. More documentation and examples [here](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/sprintf): 

```{r}
# me, slowly making sense of this:

p = 2

paste0 ("warn_listing/page", sprintf( "%03d", p ) ) 

```

```{r}

# test the loop outside a loop

url <- "https://www.azjobconnection.gov/ada/mn_warn_dsp.cfm?securitysys=on&start_row=1&max_rows=25&orderby=employer&choice=1"
new_start_row <- ( (p-1) * 25 + 1 )
new_url <- str_replace(url,
                       "(start_row=)(1)", # regex pattern 
                       paste0( "\\1", new_start_row ) ) # replacement 
new_url

```


We only need to change `start_row` when we're looping between pages. Everything else stays the same.

```{r, eval = FALSE}

url <- "https://www.azjobconnection.gov/ada/mn_warn_dsp.cfm?securitysys=on&start_row=1&max_rows=25&orderby=employer&choice=1"

# Get the pages: 
      for (p in  2:16) {
          
          # set variables
          new_start_row <- ( (p-1) * 25 + 1 ) 
          max_rows <- 25
          
          # build the url from there by using str_replace for start_row=1 
          # something like this:
          new_url <- str_replace( url, 
                                  "(start_row=)(1)", # regex pattern
                                  paste0("\\1", new_start_row ) ) # replacement 
          download.file (new_url, 
                         paste0 ("warn_listing/page", 
                                  sprintf("%03d", p),
                                 ".html"),
                         method="auto" )
          Sys.sleep(1)
          
      }

```

# Parsing the pages

Start by reading in a single page using rvest:

```{r}

# using html_table - which doesn't help that much, bc we still need to parse the link

my_html <- read_html( "warn_listing/page001.html" )
html_nodes( my_html, "table" ) # there are 4 tables
html_nodes( my_html, "table")[[2]]  
warns <- html_table( html_nodes( my_html, "table")[[2]] )

```

Note: html_table looks cool and at first glance appears to save a lot of work. But since we need to get the URL in each row manually anyway, it doesn't save us any time - we may as well parse everything manually.

Now use Sarah's prescribed method:

```{r}

# using xpath
all_rows <- html_nodes( my_html, xpath='//div[2]/table/tr' )

# iterate through first three rows to get a feel for it. 
html_children( all_rows[ 1 ] ) # row 1 has the column headers
html_children( all_rows[ 2 ] ) # row 2 has ... I'm not sure, but looks like a header column/formatting
html_children( all_rows[ 3 ] ) # now we see <td> elements, or cells in a table.

# since we're defining our own headers, we can scrap the first two rows
n = length( all_rows )
data_rows <- all_rows[ 3:n ]

#now get the rest of the info. 
row_data[ 1, 4 ] <- html_node( data_rows[ 1 ], xpath="td[1]" ) %>% html_text( )

```


Beginning step of parsing: create a table to hold the data!
```{r}

row_data <- tibble(
  
  page_num = character( ),
  row_num = numeric( ),
  full_link = character( ),
  company = character( ),
  co_hq_city = character( ),
  co_hq_zip = character( ),
  labor_area = character( ),
  notice_date_txt = character( )
  
)

```

Parse the first row alone to see how this works.

```{r}

parent_url <- "https://www.azjobconnection.gov/ada/"

row_data[ 1, 1 ] <- "page001.html"    # in a loop, I'll update using sprintf

row_data[ 1, 2 ] <- 1                 # I'll update using my p-value

row_url <- html_node( data_rows[1], xpath = "td[1]/span/a/@href" ) %>% 
  html_text( ) 

# strip out the page ID from a very clunky URL 

row_data[ 1, 3 ]  <- paste0( parent_url, 
          str_extract( row_url, "^.{16}(id=)\\d{1,4}" ) )
  
row_data[ 1, 3 ]

#now get the rest of the info.  
row_data[ 1, 4 ] <- html_node( data_rows[ 1 ], xpath ="td[1]" ) %>% html_text( )
row_data[ 1, 5 ] <- html_node( data_rows[ 1 ], xpath ="td[2]" ) %>% html_text( )
row_data[ 1, 6 ] <- html_node( data_rows[ 1 ], xpath ="td[3]" ) %>% html_text( )
row_data[ 1, 7 ] <- html_node( data_rows[ 1 ], xpath ="td[4]" ) %>% html_text( )
row_data[ 1, 8 ] <- html_node( data_rows[ 1 ], xpath ="td[5]" ) %>% html_text( )

# check it out

row_data[ 1, ]

```

Got it. I think I understand enough to write it (with much trial and error) in a continuous loop.

# Parsing the data in a loop

```{r}

# define parent url
parent_url <- "https://www.azjobconnection.gov/ada/"

# define data frame
row_data <- tibble(
  
  page_num = character( ),
  row_num = numeric( ),
  full_link = character( ),
  company = character( ),
  co_hq_city = character( ),
  co_hq_zip = character( ),
  labor_area = character( ),
  notice_date_txt = character( )
  
)

# iterate through 16 html pages
for ( p in 1:16 ) {
  
  # parse page items
  my_html = read_html( paste0 ("warn_listing/page", sprintf( "%03d", p ), ".html" ) )
  
  all_rows <- html_nodes( my_html, xpath = '//div[2]/table/tr' )
  
  n = length( all_rows )
  
  data_rows = all_rows[ 3:n ]
  
  new_start_row <- ( (p-1) * 25 + 1 ) 
  
  num_data_rows <- length( data_rows )
  
  # parse data_rows - begins at a multiple of 25 (plus 1)
  for ( i in 1:num_data_rows ) {
  
    j <- new_start_row + i - 1
    
    row_data[ j, 1 ] <- paste0 ("page", sprintf( "%03d", p ), ".html")
      
    row_data[ j, 2 ] <- j
      
    # quick two-step URL extraction
    row_url <- html_node( data_rows[ i ], xpath = "td[1]/span/a/@href" ) %>% html_text( )
    row_data[ j, 3 ]  <- paste0( parent_url, str_extract( row_url, "^.{16}(id=)\\d{1,4}" ) )
      
    row_data[ j, 4 ] <- html_node( data_rows[ i ], xpath = "td[1]" ) %>% html_text( )
      
    row_data[ j, 5 ] <- html_node( data_rows[ i ], xpath = "td[2]" ) %>% html_text( )
      
    row_data[ j, 6 ] <- html_node( data_rows[ i ], xpath = "td[3]" ) %>% html_text( )
      
    row_data[ j, 7 ] <- html_node( data_rows[ i ], xpath = "td[4]" ) %>% html_text( )
      
    row_data[ j, 8 ] <- html_node( data_rows[ i ], xpath="td[5]" ) %>% html_text( )
      
  
  } 
  
  # ta-da!
  
}

```

A little bit of clean-up:
```{r}

# add an id column + make the date column a datetime datatype

warn_data <- row_data %>% 
    mutate( id = str_extract( full_link, "\\d{1,4}"),
            notice_date = mdy( notice_date_txt ) ) %>% 
    select( -notice_date_txt )

write_csv( warn_data, "/Users/helenwieffering/Desktop/data-projects/cronkite-data-journalism/warn-layoffs/warn_data.csv" )

```


Hooray! Half is done. Now let's prep for the next step: saving the individual WARN URLs.

#Part II - downloading every WARN notice

```{r}

# create a table with ONLY the links we need
company_url <- row_data %>% select( full_link )

# check that all the links are unique -- they are
nrow( company_url )
company_url %>% summarise( n_urls = n_distinct(full_link) )
```

Write a loop to download these urls.

```{r, eval = FALSE}

end <- nrow( company_url )

# we'll only need to iterate through our list of links, and save the page with a filepath that reflects the id
# since there are 379 requests, at 1 second each it would take more than an hour. Sarah suggested I could likely cut it down to 0.5. I'm going to be conservative and try 0.75.

for ( u in 1:end ) {

  my_url <- company_url[ u, 1 ] %>% as.character( )
  
  id <- str_extract( my_url, '(id=)\\d{1,4}' )
  
  download.file( my_url, 
                 paste0 ("company_urls/warn/", id ), method="auto")
  
  Sys.sleep(0.75)
  
}


```

#Parsing the WARN notices

Parse the first page to see how this works. It seems like, in this case, html_table will be great! (Since we don't need to do any extra manual parsing.) All I'll do is pivot the data once it's parsed, and add an id column.
```{r}

my_html <- read_html( "company_urls/id=94" )

my_nodes <- html_nodes( my_html, "table" )[[2]] # there are 2 tables

html_table( html_nodes( my_html, "table")[[2]] ) %>% 
  pivot_wider( names_from = X1, values_from = X2 )

```

One additonal challenge is that I can't walk through the pages in a sequential order as I did before. I get around that using list.files( ), which lets me iterate through all the files in the folder.

```{r}

# define data frame
warn_detail <- tibble(
  
  page_name = character( ),
  row_num = numeric( ),
  id = character( ), # there are no leading zeros, so treat it as a numeral
  company = character( ),
  co_st_address = character( ),
  co_hq_city = character( ),
  co_hq_state = character( ),
  co_hq_zip = character( ),
  num_employees_affected = numeric( ),
  other_notes = character( )
)

# create a list of files
# DON'T RUN if you are not on Helen's laptop
file_list <- list.files( path="/Users/helenwieffering/Desktop/data-projects/cronkite-data-journalism/warn-layoffs/company_urls/warn" )

n <- length( file_list )

# loop through every scraped file to parse the data
for (i in 1:n ) {
  
  file_name <- file_list[ i ] %>% as.character( )
  
  my_html <- read_html( paste0( "company_urls/warn/", file_name ) )
  
  my_table <- html_table( html_nodes( my_html, "table" )[[2]])
  
  table_length <- nrow( my_table )
  
  if( table_length < 7 ) {
    r = tribble( 
      ~X1, ~X2,
      "Other Notes", "" )
    
    my_table <- bind_rows( my_table, r )
  }
  
  warn_row <- my_table %>%
    pivot_wider( names_from = X1, values_from = X2 ) %>%
    mutate(
      id = str_extract(file_name, "\\d{1,4}"),
      page_name = file_name,
      row_num = i,
      num_employees_affected = `Number of employees affected` %>% as.numeric( ),
      other_notes = `Other Notes`
    ) %>% 
    # re-order and rename columns to match data-frame
    select(
      page_name, 
      row_num, 
      id, 
      company = `Company name`,
      co_st_address = `Street address`,
      co_hq_city = `City`,
      co_hq_state = `State`,
      co_hq_zip = `Zip code`,
      num_employees_affected,
      other_notes
    )
  
  warn_detail <- bind_rows( warn_detail, warn_row  )
  
}

# woohoo!

# make sure to save your results
#write_csv( warn_detail, "/Users/helenwieffering/Desktop/data-projects/cronkite-data-journalism/warn-layoffs/warn_detail.csv" )

```

#Review

Give it a look:
```{r}

glimpse( warn_data )
glimpse( warn_detail )

warn_data_complete <- warn_data %>% 
  left_join( warn_detail, by = c( "id" = "id" ) ) %>% 
  select(
    id,
    results_page = page_num, # page number of results on WARN website
    page_name, # id number within URL for detailed WARN
    full_link,
    company = company.x,
    co_hq_city = co_hq_city.x,
    co_hq_state,
    co_hq_zip = co_hq_zip.x,
    labor_area,
    notice_date,
    num_employees_affected
  )

glimpse( warn_data_complete )

write_csv( warn_data_complete, "/Users/helenwieffering/Desktop/data-projects/cronkite-data-journalism/warn-layoffs/warn_data_complete.csv" )

# when analyzing, make sure to note that the data was downloaded on March 30, 2020.

```
 
# Preview of analysis

```{r}

warn_by_yr <- warn_data_complete %>%
  mutate( m = month( notice_date ),
          y = year( notice_date ),
          mo_yr = paste(m, y, sep = "-" ) ) %>% 
  group_by( m, y, mo_yr ) %>%
  summarise(
    notices = n( ),
    distinct_companies = n_distinct( company ),
    employees_affected = sum( num_employees_affected ),
  ) %>%
  arrange( desc( y ), desc( m ) )




```

Who was laid off in 2019?
```{r}

warn_data_complete %>% 
  filter( year( notice_date ) == 2019 ) %>% 
  mutate( m = month( notice_date ),
          y = year( notice_date ),
          mo_yr = paste(m, y, sep = "-" ) ) %>% 
  select( company, num_employees_affected, mo_yr, notice_date ) %>% 
  arrange( desc( num_employees_affected ) )

```

Some heartbreakers:

Tons of Safeway employees
Arizona Daily Star (59)
Compass USA - St Mary' Hospital


